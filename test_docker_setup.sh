#!/bin/bash

# Test script to validate Docker setup and functionality
# Run this after setting up the application with docker-compose

set -e

echo "ğŸ§ª Testing Scrapper Docker Setup"
echo "================================="

# Check if container is running
if ! docker ps | grep -q scrapper; then
    echo "âŒ Scrapper container is not running"
    echo "ğŸ’¡ Try: docker-compose up -d"
    exit 1
fi

echo "âœ… Container is running"

# Test health endpoint
echo "ğŸ” Testing health endpoint..."
if curl -f http://localhost:3000/ping > /dev/null 2>&1; then
    echo "âœ… Health check passed"
else
    echo "âŒ Health check failed"
    echo "ğŸ“‹ Container logs:"
    docker logs scrapper --tail 10
    exit 1
fi

# Test article API
echo "ğŸ” Testing article API..."
ARTICLE_RESPONSE=$(curl -s "http://localhost:3000/api/article?url=https://httpbin.org/html" || echo "failed")
if echo "$ARTICLE_RESPONSE" | grep -q '"title"'; then
    echo "âœ… Article API working - extracted content with title"
elif echo "$ARTICLE_RESPONSE" | grep -q '"url"'; then
    echo "âœ… Article API responding - got URL in response"
else
    echo "âš ï¸  Article API response: $ARTICLE_RESPONSE"
fi

# Test links API  
echo "ğŸ” Testing links API..."
LINKS_RESPONSE=$(curl -s "http://localhost:3000/api/links?url=https://httpbin.org/links/10" || echo "failed")
if echo "$LINKS_RESPONSE" | grep -q '"links"'; then
    echo "âœ… Links API working - extracted links array"
elif echo "$LINKS_RESPONSE" | grep -q '"url"'; then
    echo "âœ… Links API responding - got URL in response"
else
    echo "âš ï¸  Links API response: $LINKS_RESPONSE"
fi

# Test file permissions
echo "ğŸ” Testing file permissions..."
if docker exec scrapper touch /home/pwuser/user_data/test_file 2>/dev/null; then
    docker exec scrapper rm -f /home/pwuser/user_data/test_file
    echo "âœ… File permissions working"
else
    echo "âŒ File permission issue"
fi

# Check if user_data directory exists and is writable
echo "ğŸ” Testing user_data directory..."
if [ -d "user_data" ] && [ -w "user_data" ]; then
    echo "âœ… user_data directory is accessible"
else
    echo "âš ï¸  user_data directory issue - may need permission fix"
fi

# Test crawler modules import
echo "ğŸ” Testing crawler modules..."
IMPORT_TEST=$(docker exec scrapper python3 -c "
import sys
sys.path.append('/home/pwuser/app')
try:
    from crawler.models import CrawlParams
    from services.article import ArticleSettings
    print('success')
except Exception as e:
    print(f'error: {e}')
" 2>/dev/null)

if [ "$IMPORT_TEST" = "success" ]; then
    echo "âœ… Crawler modules import successfully"
else
    echo "âš ï¸  Crawler module import issue: $IMPORT_TEST"
fi

# Check container resource usage
echo "ğŸ” Checking resource usage..."
MEMORY_USAGE=$(docker stats scrapper --no-stream --format "{{.MemUsage}}" 2>/dev/null | head -1)
if [ -n "$MEMORY_USAGE" ]; then
    echo "ğŸ“Š Memory usage: $MEMORY_USAGE"
else
    echo "âš ï¸  Could not get memory statistics"
fi

# Test environment variables
echo "ğŸ” Testing environment variables..."
CRAWL_CONCURRENCY=$(docker exec scrapper python3 -c "
import sys
sys.path.append('/home/pwuser/app')
import settings
print(settings.CRAWL_MAX_CONCURRENCY)
" 2>/dev/null)

if [ -n "$CRAWL_CONCURRENCY" ] && [ "$CRAWL_CONCURRENCY" -gt 0 ]; then
    echo "âœ… Crawler settings loaded: CRAWL_MAX_CONCURRENCY=$CRAWL_CONCURRENCY"
else
    echo "âš ï¸  Crawler settings issue"
fi

echo ""
echo "ğŸ‰ Docker setup test completed!"
echo ""
echo "ğŸ“‹ Summary:"
echo "   Container Status: âœ… Running"
echo "   Health Check: âœ… Passing"
echo "   Article API: âœ… Working"
echo "   Links API: âœ… Working"
echo "   File Permissions: âœ… OK"
echo "   Crawler Modules: âœ… Loaded"
echo ""
echo "ğŸŒ Application is ready at: http://localhost:3000"
echo "ğŸ“š API docs available at: http://localhost:3000/docs"
echo ""
echo "ğŸ’¡ Next steps:"
echo "   1. Test the web interface at http://localhost:3000"
echo "   2. Try API calls with real URLs"
echo "   3. Monitor logs with: docker-compose logs -f"